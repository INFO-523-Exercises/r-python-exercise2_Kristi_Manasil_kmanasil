---
title: "Data Preprocessing in R"
author: "Kristi Manasil"
gitid: "kmanasil"
format: html
editor: visual
---

# **Data Preprocessing in R**

# **Goal**

Practice basic R commands/methods for descriptive data analysis. If you are already familiar with some of the commands/methods, practice the ones new to you.

**Note**: copying and pasting early in learning will not produce the results you are looking for, and will catch up to you eventually.

## **Submission**

Please submit `.r`, `.rmd`, or `.qmd` files ONLY.

# **Installing required packages**

```{r}
# Required package for quick package downloading and loading 
if (!require(pacman))  
  install.packages("pacman")

library(pacman)

p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr,
       formattable) 
```

# **Loading data**

### **CSV files (`.csv`)**

```{r}
# load the data
data <- read_csv(here("data", "x.csv"))

# inspect it
data |> glimpse()
```

The `|>` is the Base R pipe as opposed to the `magrittr` pipe `|>`. The `|>` pipe can be utilized for most functions in R, while the `|>` pipe is more restricted towards the `tidyverse`. (I am using here but I prefer the %\>%)

### **Tab separated values (`x.tsv`)**

```{r}
# load the data
data <- read_delim(here("data", "x.tsv"))

# inspect
data |> glimpse()
```

## **Importing data from MySQL database**

First connect to a database in a MySQL database management system, then query tables in the database to obtain desired dataset.

```{r}
# obtain driver for MySQL, drivers available for other DBMS
drv <- dbDriver("MySQL")

```

Get a connection to my local mysql database `etcsite_charaparser`. If you don't have a local database handy, you can skip this exercise and just get the idea on what it takes to connect to a db

I do not have a local db handy. I am doing the steps to get the idea but they will not run on my side. And hopefully when I have time I can try to get a local db and try this.

### **Using `dplyr` instead**

```{r}
install.packages("dbplyr") #install but don’t run library() on this dbplyr.
```

### **Obtain a connection**

```{r}
#con <- src_mysql("etcsite_charaparser", user = "termsuser", password = "termspassword", host = "localhost")
```

Get an entire table as `tbl`

```{r}

#allwords <- tbl(con, "1_allwords")
#allwords
```

Next you can use methods in dplyr to select and filter if needed. DBI library allows you to use SQL query to select your data from db tables while dplyr library methods can be used when you don't even know SQL language.

Importing extremely large dataset, consider `data.table` package's `fread()`, or `iotools` package's `read.csv.raw()`.

# **Data Cleaning**

## **Wide vs. long format**

Read data in wide format

```{r}
# wide
wide <- read_delim(here("data", "wide.txt"), delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

The wide format uses the values (`Math`, `English`) of variable `Subjects` as variables.

The long format should have `Name`, `Subject`, and `Grade` as variables (i.e., columns).

```{r}
# make it long
long <- wide %>% 
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

## **Long to wide, use `spread()`**

```{r}
# back to wide with pivot
wide <- long %>% 
  pivot_wider(names_from = Subject, values_from = Grade)

wide
```

## **Split a column into multiple columns**

Split `Degree_Year` to `Degree` and `Year`

```{r}
# use separate
clean <- long %>% 
  separate(Degree_Year, c("Degree", "Year"), sep = "_")

clean
```

## **Handling date/time and time zones**

```{r}
# add lubridate
install.packages("lubridate")
library(lubridate)
```

Convert dates of variance formats into one format:

```{r}
#convert to year-month-day format
mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates)

clean.dates
```

Extract day, week, month, year info from dates:

```{r}
data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

Time zone:

```{r}
date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")
```

Convert to Phoenix, AZ time:

```{r}
with_tz(date.time, tz="America/Phoenix")
```

Change the timezone for a time:

```{r}
force_tz(date.time, "Turkey")
```

Check available time zones:

```{r}
OlsonNames()
```

That is cool.

## **String Processing**

Common needs: `stringr` package

Advanced needs: `stringi` package

The following code shows the use of functions provided by stringr package to put column names back to a dataset fetched from <http://archive.ics.uci.edu/ml/machine-learning-databases/audiology/>

```{r}
# load libraries
library(dplyr)
library(stringr)
library(readr)
```

Fetch data from a URL, form the URL using string functions:

```{r}
# copy and paste so I dont mess up the address
uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"

dataset <- "audiology/audiology.standardized"
```

`str_c`: string concatenation:

```{r}
dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

Read the data

```{r}
# read in data
data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

```{r}
# check the dimensions of df
dim(data)
```

Read the name file line by line, put the lines in a vector:

```{r}
lines <- read_lines(url(namesF))

lines %>% head()
```

Examine the content of lines and see the column names start on line 67, ends on line 135. Then, get column name lines and clean up to get column names:

```{r}
names <- lines[67:135]
names
```

Observe: a name line consists two parts, name: valid values. The part before `:` is the name.

```{r}
# split on regular expression pattern ":"
# this function returns a matrix
names <- str_split_fixed(names, ":", 2)
names
```

Take the first column, which contains names:

```{r}
names <- names[,1]

names
```

Now clean up the names: trim spaces, remove `()`:

```{r}
# we use a pipe, and another reg exp "\\(|\\)", \\ is the escape.

names <-str_trim(names) |> str_replace_all("\\(|\\)", "") 
names
```

Finally, put the columns to the data:

*Note:* data has 71 rows but we only has 69 names. The last two columns in data are identifier and class labels. So we will put the 69 names to the first 69 columns.

```{r}
colnames(data)[1:69] <- names
data
```

Rename the last two columns:

```{r}
colnames(data)[70:71] <- c("id", "class")
data
```

DONE!

## **Dealing with unknown values**

Remove observations or columns with many NAs:

```{r}
# load library
library(dplyr)

# find rows with missing values
missing.value.rows <- data %>% 
  filter(!complete.cases(data))
missing.value.rows
```

196 out of 200 rows contain an NA!

How many NAs in each row? Apply a temporary function to the rows ("1", if to columns use "2") of data. This function counts the number of NAs in a row. If is.na(x) is TRUE (equivalent to 1), the sum of the booleans is then the count.

```{r}

data <- data %>%
  mutate(na_count = rowSums(is.na(data)))
data
```

Maximum missing values in a row is 7, out of 69 dimensions, so they are not too bad.

Examine columns: how many NAs in each variable/column?

```{r}
data %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
  arrange(na_count)
```

`bser` variable has 196 NAs. If this variable is considered not useful, given some domain knowledge, we can remove it from the data. From View, I can see bser is the 8th column:

```{r}
data.bser.removed <- data %>%
  select(-8) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))

data.bser.removed
```

`matches` function can also help you find the index of a `colname` given its name:

```{r}
data <- data %>% 
  select(-matches("bser"))
```

### **Mistaken characters**

Because R decides the data type based on what is given, sometimes, R's decision may not be what you meant. In the example below, because of a missing value `?`, R makes all other values in a vector 'character'. Parse_integer can be used to fix this problem. 

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

```{r}
fixed <- parse_integer(mistaken, na = '?')
fixed
```

```{r}
class(fixed)
```

### **Filling unknowns with most frequent values**

Always take cautions when you modify the original data. 

!!!Modifications must be well documented (so others can repeat your analysis) and well justified!!!

```{r}
# load more libraries
install.packages("DMwR2")
library(DMwR2)
data(algae, package = "DMwR2")
algae[48,]
```

`mxPH` is unknown. Shall we fill in with mean, median or something else?

```{r}
# plot a QQ plot of mxPH
#install.packages("car")
library(car)
ggplot(algae, aes(sample = mxPH)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of mxPH") 
```

The straight line fits the data pretty well so `mxPH` is normal, use mean to fill the unknown.

```{r}
algae <- algae %>% 
  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
algae
```

What about attribute `Chla`?

```{r}
ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla") 
```

```{r}
median(algae$Chla, na.rm = TRUE)
```

```{r}
mean(algae$Chla, na.rm = TRUE)
```

Obviously, the mean is not a representative value for `Chla`. For this we will use median to fill all missing values in this attribute, instead of doing it one value at a time:

```{r}
# replace na with median value
algae <- algae %>% 
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))
```

### **Filling unknowns using linear regression**

This method is used when two variables are highly correlated. One value of variable A can be used to predict the value for variable B using the linear regression model.

First let's see what variables in algae are highly correlated:

```{r}
algae_numeric <- algae[, 4:18] %>%
  drop_na()  # Removes rows with NA values

cor_matrix <- algae_numeric %>%  correlate() %>%  plot()
```

```{r}
cor_matrix
```

We can see from the matrix, `PO4` and `oPO4` are correct with a confidence level of 90%.

Next, we find the linear model between `PO4` and `oPO4`:

```{r}
#this is a method provided that selects the observations with 20% or move values as NAs. 
algae <- algae %>%
  filter(rowSums(is.na(.)) / ncol(.) < 0.2)

m = lm(PO4 ~ oPO4, data = algae)
lm(formula = PO4 ~ oPO4, data = algae)
```

Check the model, is it good? See <http://r-statistics.co/Linear-Regression.html>

```{r}
m %>% summary()
```

```{r}
m %>% 
  summary() %>%  
  tidy()
```

If a good model, coefficients should all be significant (reject Ho coefficience is 0), Adjusted R-squared close to 1 (0.8 is very good).

F-statistics p-value should be less than the significant level (typically 0.05).

While R-squared provides an estimate of the strength of the relationship between your model and the response variable, it does not provide a formal hypothesis test for this relationship.

The [F-test of overall significance](https://datamineaz.org/slides/week4/rexercise2#0) determines whether this relationship is statistically significant.

This model is good. We can also assess the fitness of the model with fitted line plot (should show the good fit), residual plot (should show residual being random).

This `lm` is `PO4 = 1.293*oPO4 + 42.897`

```{r}
algae$PO4
```

PO4 for observation 28 can then be filled with predicated value using the model

```{r}
algae <- algae %>%
  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))
```

```{r}
res = resid(m)

oPO4_reduced <- algae %>%
  filter(row_number() != 28) %>%
  pull(oPO4)
```

```{r}
ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "oPO4",
    y = "residuals",
    title = "Residual Plot"
  )
```

If there are more PO4 cells to fill, we can use `sapply()` to apply this transformation to a set of values

Create a simple function `fillPO4`:

```{r}
fillPO4 <- function(x) {
  if_else(is.na(x), 42.897 + 1.293 * x, x)
}
#if x is not NA, return 42.897+1.293*x 
```

```{r}
algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
```

Apply calls `fillPO4` function repeatedly, each time using one value in `algae[is.na(algae$PO4), "oPO4"]` as an argument.

You can perform a similar operation on subsets of observations. For example, if `PO4` is missing for data collected in **summer** and season affects `PO4` values, you may consider to use a linear model constructed with **summer** data only to obtain a more accurate prediction of `PO4` from `oPO4`.

### **Filling unknowns by exploring similarities among cases**

```{r}
data(algae, package="DMwR2")
algae <- algae[-manyNAs(algae), ] 
```

`DM2R2` provides a method call `knnImputation()`. This method use the Euclidean distance to find the ten most similar cases of any water sample with some unknown value in a variable, and then use their values to fill in the unknown.

We can simply calculate the median of the values of the ten nearest neighbors to fill in the gaps. In case of unknown nominal variables (which do not occur in our algae data set), we would use the most frequent value (the mode) among the neighbors. The second method uses a weighted average of the values of the neighbors.

The weights decrease as the distance to the case of the neighbors increases.

```{r}
algae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples


data(algae, package="DMwR2") #get data again so there are unknown values
algae <- algae[-manyNAs(algae), ] 
algae <- knnImputation(algae, k = 10, meth="median") #use the median of k most similar samples
```

To see what is in `knnImputation()` (You are not required to read and understand the code):

```{r}
getAnywhere(knnImputation())
```

# **Scaling and normalization**

Normalize value using `scale()`

Normalize values in penguins dataset:

```{r}
# load libraries
library(dplyr)
#library(palmerpenguins)

data("penguins")
```

```{r}
# select only numeric columns
penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# normalize numeric columns
penguins_norm <- scale(penguins_numeric)

# convert back to data frame and add species column
peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)

# because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.

summary(penguins)
```

```{r}
summary(peng.norm)
```

`scale()` can also take an argument for center and an argument of scale to normalize data in some other ways, for example,

```{r}
max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)

max
```

```{r}
min
```

```{r}
# min-max normalization
penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

penguin_scaled <- cbind(penguins_norm, species = penguins$species)

summary(penguin_scaled)
```

## **Discretizing variables (binning)**

The process of transferring continuous functions, models, variables, and equations into discrete counterparts

Use `dlookr`'s `binning(type = "equal")` for equal-length cuts (bins)

Use `Hmisc`'s `cut2()` for equal-depth cuts

Boston Housing data as an example

```{r}
data(Boston, package="MASS")
summary(Boston$age)
```

```{r}
#create 5 bins and add new column newAge to Boston
Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") 
summary(Boston$newAge)
```

```{r}
#add labels
Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal") 

summary(Boston$newAge)
```

### **Equal-depth**

```{r}
install.packages("Hmisc")
library(Hmisc)

#create 5 equal-depth bins and add new column newAge to Boston
Boston$newAge <- cut2(Boston$age, g = 5) 

table(Boston$newAge)
```

### **Assign labels**

```{r}
Boston$newAge <- factor(cut2(Boston$age, g = 5), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

Plot an equal-width histogram of width 10:

```{r}
#seq() gives the function for breaks. The age ranges from 0 – 101.
hist(Boston$age, breaks = seq(0, 101,by = 10)) 
```

or, use `ggplot2`!

```{r}
library(ggplot2)

Boston %>% 
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

## **Decimal scaling**

```{r}
data <- c(10, 20, 30, 50, 100)

#nchar counts the number of characters
(nDigits = nchar(max(abs(data)))) 
```

```{r}
# change to decimal
(decimalScale = data / (10^nDigits))
```

### **Smoothing by bin mean**

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate data into bins of depth 3
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

Find the average for each bin

```{r}
# find average
(bin_means = apply(bins, 1, FUN = mean))
```

Replace values with their bin mean:

```{r}
# use loop to change value to average
for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

# **Variable correlations and dimensionality reduction**

## **Chi-squared test**

H0: (Prisoner's race)(Victim's race) are independent.

data (contingency table):

```{r}
racetable = rbind(c(151,9), c(63,103))
test1 = chisq.test(racetable, correct=F)
test1
```

p-value is less than 0.05: chance to get X-squared value of 115.01 assuming H0 is true is very slim (close to 0), so reject H0.

## **Loglinear model**

Extending chi-squared to more than 2 categorical variables.

Loglinear models model cell counts in contingency tables.

Adapted from <https://data.library.virginia.edu/an-introduction-to-loglinear-models/>:

Data from Agresti (1996, Table 6.3) modified. It summarizes responses from a survey that asked high school seniors in a particular city whether they had ever used alcohol, cigarettes, or marijuana. A categorical variable age is added.

We're interested in how the cell counts in the contingency table depend on the levels of the categorical variables.

Get the data in. We will be analyzing cells in contingency tables, so use a multi-dimensional array to hold the data.

```{r}
# make the data
seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

Observe how data is saved in the 2x2x2x2 array:

```{r}
seniors
```

Next, do loglinear modeling using the glm function (generalized linear models).

We need to convert the array to a table then to a data frame.

Calling `as.data.frame` on a table object in R returns a data frame with a column for cell frequencies where each row represents a unique combination of variables.

```{r}
seniors.tb <- as.table(seniors)
seniors.tb
```

```{r}
# change table to df
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

Next, we model Freq (this is the count in the contingency table) as a function of the three variables using the glm function. Set `family = poisson` because we are assuming independent counts. 

Poisson distribution: discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event.

Our H0 is the four variables are independent of one another.

We will test the saturated model first (including all variables and all two-way and three-way interactions), because it will show the significance for all variables and their interactions

Use `*` to connect all variables to get a saturated model, which will fit the data perfectly. Then we will remove effects that are not significant.

```{r}
mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
summary(mod.S4)
```

*Note*: "Residual deviance" indicates the fitness of the model to the data. A good fit would have residual deviance less than or close to its degree of freedom. That is the case for the saturated model, which is expected.

Then look at "Coefficients" (these are the lamdas). Many of them are not significant (\*, \*\*, \*\*\* indicates significant lamdas)

By examining those insignificant effects, we see they all involve `age`.

Now lets' remove age and re-generate a model with the remaining three variables.

```{r}
mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
summary(mod.S3)
```

We see the model fits well, and most effects are significant now.

The insignificant one is the 3-way interaction among the three factors

For data reduction, we are done -- we removed `age` variable. Because all cigarette, marijuana, and alcohol effects are significant, we can't remove any of these from the data set, even though the 3-way interaction is not significant.

For data modeling, we can remove the 3-way interaction by testing "`Freq ~ (cigarette + marijuana + alcohol)^2`" (`^2` tells glm to check only two way interactions).

```{r}
mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
summary(mod.3)
```

Now compare the fitted and observed values and see how well they match up:

```{r}
cbind(mod.3$data, fitted(mod.3))
```

They fit well?

## **Correlations**

```{r}
library(tidyr) 

# data manipulation
penguins_numeric %>%  
  drop_na() %>% 
  correlate()
```

`bill_length_mm` and `flipper_length_mm` are highly negatively correlated, `body_mass_g` and `flipper_length_mm` are strongly positively correlated as well.

## **Principal components analysis (PCA)**

```{r}
pca.data <- penguins %>% 
  drop_na() %>% 
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

```{r}
# pca result is a list, and the component scores are elements in the list
head(pca$scores) 
```

If we are happy with capturing 75% of the original variance of the cases, we can reduce the original data to the first three components.

Component scores are computed based on the loading, for example:

``` comp3 = 0.941*bill_length_mm + 0.144*``bill_depth_mm``- 0.309*flipper_length_mm ```

```{r}
penguins_na <- penguins %>% 
  drop_na()

peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

head(peng.reduced)
```

Now we can use `peng.reduced` data frame for subsequent analyses:

Haar Discrete Wavelet Transform, using the example shown in class

Output:

|     |                                                                                                         |
|-----|---------------------------------------------------------------------------------------------------------|
| W:  | A list with element i comprised of a matrix containing the ith level wavelet coefficients (differences) |
| V:  | A list with element i comprised of a matrix containing the ith level scaling coefficients (averages).   |

```{r}
install.packages("wavelets")
library(wavelets)
```

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
#with 8-element vector, 3 level is the max.
wt <- dwt(x,filter="haar", n.levels = 3) 
wt
```

Why aren't the W and V vectors having the same values as shown in class?

Because in class, use simply taking the average and differences/2, in the default Haar, the default coefficients are sqrt(2)/2, see the section in bold above.

Reconstruct the original:

```{r}
idwt(wt)
```

Obtain transform results as shown in class, use a different filter:

```{r}
xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
xt
```

Reconstruct the original:

```{r}
idwt(xt)
```

# **Sampling**

```{r}
set.seed(1)
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

## **Simple random sampling, without replacement:**

```{r}
sample(age, 5)
```

## **Simple random sampling, with replacement:**

```{r}
sample(age, 5, replace = TRUE)
```

## **Stratified sampling**

```{r}
library(dplyr)
set.seed(1) #make results the same each run
summary(algae)
```

```{r}
sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

## **Cluster sampling**

```{r}
install.packages("sampling")
library(sampling)
age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

```{r}
ageframe <- data.frame(age)
ageframe$condition <- s$cluster # add cluster label as condition
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

# **Handling Text Datasets**

```{r}
pacman::p_load(tm, SnowballC)

data <- read.csv(here::here("data", "Emails.csv"), stringsAsFactors = FALSE)
docs <- Corpus(VectorSource(data$RawText))
mode(docs)
```

## **Inspect a document**

I had to add text to text.txt as it was blank. The emails.csv was to large to process in posit - so I added a doc with similar text. But I had to change the line below to 1 as 20 threw an out of bounds error.

```{r}
# add text to text.txt will use that
docs[[20]]
```

## **Preprocessing text**

```{r}
docs <- docs %>% 
         tm_map(removePunctuation) %>% 
         tm_map(content_transformer(tolower)) %>%  #to lower case
         tm_map(removeNumbers) %>% 
         tm_map(removeWords, stopwords("en")) %>%  #stopwords, such as a, an.
         tm_map(stripWhitespace) %>% 
         tm_map(stemDocument) #e.g. computer -> comput
```

```{r}
content(docs[[20]])
```

Convert text to a matrix using `TF*IDF scores` (see `TF*IDF` scores in Han's text):

```{r}
DTData <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
DTData
```

```{r}
inspect(DTData[1:2, 1:5])
```

```{r}
TDData <- TermDocumentMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
inspect(TDData[1:2, 1:5])
```

## **Explore the dataset**

```{r}

findFreqTerms(TDData, lowfreq = 75, highfreq = 1000)
```

```{r}
findAssocs(TDData, terms = "bill", corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("bill"), corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("schedul"), corlimit = 0.3)
```

## **Create a word cloud**

```{r}
install.packages("wordcloud")
install.packages("RColorBrewer")
library(wordcloud)
```

```{r}
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```

```{r}
png(file = "wordCloud.png", width = 1000, height = 700, bg= "grey30")

wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

`dev.off()` closed the `.png` file, now the current display is the default display in RStudio. Use `dev.list()` to find the graphics devices that are active, repeatedly use `dev.off()` to close devices that not needed. R Studio is the default display. When all other devices are closed, the default display is used.

Output the graph to the default display in RStudio

```{r}
wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

Can you remove hrodclintonemailcom and redo the word cloud? I am honestly not sure that is even in my word cloud

Sometimes you need to one-hot encoding a section of a dataframe. You can do it by using onehot package.

```{r}
install.packages("onehot")
library(onehot)
d <- data.frame(language=c("javascript", "python", "java"), hours=c(10, 3, 5) )
d$language <- as.factor(d$language) #convert the column to be encoded to Factor
encoded <- onehot(d)
new_d <- predict(encoded, d)
new_d
```

One hot encoding for data frame with multi-value cells (`language = "javascript, python"`)

```{r}
install.packages("qdapTools")
library(qdapTools)
d <- data.frame(language=c("javascript, python", "java"), hours = c(3, 5) )
d
```

```{r}
dlist <- as.list(d)
new_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), ", ")))) 

new_d
```

# $$ADVANCED$$

Exercises on your data set:

## **Load the Examine a Data Set**

I am using the ferris wheel data from TidyTuesday : <https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-08-09/wheels.csv>

1.  What attributes are there in your data set?

    ```{r}
    ferris_wheel_data <- read.csv(here("data", "wheels.csv"))

    ferris_wheel_data %>% glimpse()
    ```

```{r}
# exclude columns that are mostly na  values
ferris_wheel_data <- ferris_wheel_data %>% select(c("name","height", "opened", "country", "location","number_of_cabins", "passengers_per_cabin", "seating_capacity", "hourly_capacity"))


# remove rows with na values
ferris_wheel_data <- ferris_wheel_data %>% 
  drop_na()
```

```{r}
# find mean of height
ferris_wheel_data$height %>%  mean()
```

```{r}
# find median
ferris_wheel_data$height %>% median()
```

```{r}
ferris_wheel_data %>% summary()
```

1.  Do you have highly correlated attributes? How did you find out about the correlations or lack of correlations?

```{r}
library(tidyr)

ferris_wheel_data %>% 
  correlate()
```

There is a strong correlation between seating_capacity and passengers_per_cabin. There is a moderate correlation between height and seating_capacity and seating_capacity and hourly_capacity.

1.  Do you have numerical attributes that you might want to bin? Try at least two methods and compare the differences.

```{r}
height = c(ferris_wheel_data$height)

(bins = matrix(height, nrow = length(height)/6, byrow = TRUE))
```

```{r}
(bin_means = apply(bins, 1, FUN = mean))
```

```{r}
for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

1.  If you have categorical attributes, use the concept hierarchy generation heuristics (based on attribute value counts) suggested in the textbook to produce some concept hierarchies. How well does this approach work for your attributes?

    I do not know how to visualize this. I would not say ther is a clear concept hierarchy in this data but you could have total capacity branching to hourly capacity then to number of cabins then to passengers per cabin.

```{r}

```
